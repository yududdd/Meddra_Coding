{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import importlib\n",
    "warnings.filterwarnings('ignore')\n",
    "from loader import Loader\n",
    "from preprocessor import Preprocessor\n",
    "from spliter import Spliter \n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "# file_name = [\"data/ae.sas7bdat\"]\n",
    "# dict_name = [\"data/meddra_dict_v21\",\"data/meddra_dict_v22\", \"data/meddra_dict_v23\"]\n",
    "dict_name = [\"data/meddra_dict_v22\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verbatim Term</th>\n",
       "      <th>LLT Name</th>\n",
       "      <th>Version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contact dermatitis</td>\n",
       "      <td>contact dermatitis</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cataracts</td>\n",
       "      <td>cataracts</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occasional lightheadedness</td>\n",
       "      <td>lightheadedness</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>occassional neurologic dizziness</td>\n",
       "      <td>dizziness</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swollen lymph nodes bilateral neck</td>\n",
       "      <td>swollen lymph nodes</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32768</th>\n",
       "      <td>right hemicolectomy</td>\n",
       "      <td>right hemicolectomy</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32769</th>\n",
       "      <td>hepatic lobectomy</td>\n",
       "      <td>liver lobectomy</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32770</th>\n",
       "      <td>colon biopsy</td>\n",
       "      <td>colon biopsy</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32771</th>\n",
       "      <td>laparoscopic low anterior resection</td>\n",
       "      <td>lower anterior resection</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32772</th>\n",
       "      <td>cat scan guided biopsy</td>\n",
       "      <td>biopsy lung</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32773 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Verbatim Term                  LLT Name  Version\n",
       "0                       contact dermatitis        contact dermatitis     17.0\n",
       "1                                cataracts                 cataracts     17.0\n",
       "2               occasional lightheadedness           lightheadedness     17.0\n",
       "3         occassional neurologic dizziness                 dizziness     17.0\n",
       "4       swollen lymph nodes bilateral neck       swollen lymph nodes     17.0\n",
       "...                                    ...                       ...      ...\n",
       "32768                  right hemicolectomy       right hemicolectomy     23.0\n",
       "32769                    hepatic lobectomy           liver lobectomy     23.0\n",
       "32770                         colon biopsy              colon biopsy     23.0\n",
       "32771  laparoscopic low anterior resection  lower anterior resection     23.0\n",
       "32772               cat scan guided biopsy               biopsy lung     23.0\n",
       "\n",
       "[32773 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.rawdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Preprocessor(loader.rawdf, loader.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "medra,raw = processor.pipe_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TERM</th>\n",
       "      <th>LLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contact dermatitis</td>\n",
       "      <td>contact dermatitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cataract</td>\n",
       "      <td>cataracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occasional lightheadedness</td>\n",
       "      <td>lightheadedness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>occassional neurologic dizziness</td>\n",
       "      <td>dizziness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swollen lymph node bilateral neck</td>\n",
       "      <td>swollen lymph nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32638</th>\n",
       "      <td>abdominal hysterectomy</td>\n",
       "      <td>abdominal hysterectomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32639</th>\n",
       "      <td>right hemicolectomy</td>\n",
       "      <td>right hemicolectomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32640</th>\n",
       "      <td>hepatic lobectomy</td>\n",
       "      <td>liver lobectomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32641</th>\n",
       "      <td>colon biopsy</td>\n",
       "      <td>colon biopsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32642</th>\n",
       "      <td>cat scan guided biopsy</td>\n",
       "      <td>biopsy lung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32643 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TERM                     LLT\n",
       "0                     contact dermatitis      contact dermatitis\n",
       "1                               cataract               cataracts\n",
       "2             occasional lightheadedness         lightheadedness\n",
       "3       occassional neurologic dizziness               dizziness\n",
       "4      swollen lymph node bilateral neck     swollen lymph nodes\n",
       "...                                  ...                     ...\n",
       "32638             abdominal hysterectomy  abdominal hysterectomy\n",
       "32639                right hemicolectomy     right hemicolectomy\n",
       "32640                  hepatic lobectomy         liver lobectomy\n",
       "32641                       colon biopsy            colon biopsy\n",
       "32642             cat scan guided biopsy             biopsy lung\n",
       "\n",
       "[32643 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, X_ls, X_testls = Spliter(raw, medra).get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              exacerbation herpes simplex\n",
       "1           painful - l foot joint big toe\n",
       "2         high creatinine level 106 umol l\n",
       "3            absent reflex lower extremity\n",
       "4        interstitial nodular opacity lung\n",
       "                       ...                \n",
       "30026                        uti infection\n",
       "30027                     burnt right hand\n",
       "30028                       hypomagnasemia\n",
       "30029                            sore limb\n",
       "30030                   allergy penicillin\n",
       "Name: TERM, Length: 30031, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30031,), (2612,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 43s, sys: 15.3 s, total: 6min 58s\n",
      "Wall time: 7min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WARNING: Time consuming\n",
    "word_to_index, index_to_word, word_to_vec_map = utils.read_emb_vecs('./ri-3gram-400-tsv/vocab.tsv', './ri-3gram-400-tsv/vectors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CLASSES=len(set(medra['LLT']))\n",
    "WINDOWS_Size=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count potential spelling errors or words cannot be found in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for record in X_ls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7752 number of individual training words NOT found in the word embedding vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(cnt) + \" number of individual training words NOT found in the word embedding vectors\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 27s, sys: 929 ms, total: 16min 28s\n",
      "Wall time: 16min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WARNING: Time consuming cell\n",
    "from spellchecker import SpellChecker \n",
    "spell = SpellChecker()\n",
    "for record in X_ls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            idx = record.index(i)\n",
    "            record[idx] = spell.correction(i)\n",
    "            \n",
    "for record in X_testls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            idx = record.index(i)\n",
    "            record[idx] = spell.correction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt2 = 0\n",
    "for record in X_ls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "#             print(i)\n",
    "            cnt2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5501 number of individual training words NOT found after spell correction and other corrections\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(cnt2) + \" number of individual training words NOT found after spell correction and other corrections\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {}\n",
    "for i, pt in enumerate(set(medra['LLT'])):\n",
    "    encoder.update({pt: i})\n",
    "    i = i + 1\n",
    "decoder = dict([(pt, i) for i, pt in encoder.items()])\n",
    "\n",
    "outfile = open(os.path.join('data', 'lltcoder.pkl'),'wb')\n",
    "pickle.dump(encoder,outfile)\n",
    "outfile.close()\n",
    "outfile = open(os.path.join('data', 'lltdecoder.pkl'),'wb')\n",
    "pickle.dump(decoder,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code are designed to test if there are any train/test not in the target meddra version. Even though, using different version to train the model, it not makes sense to includes every version in the output since the output is version specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra in test:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"extra in test: \",[i for i in set(Y_test.tolist()) if i not in set(medra['LLT'].tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra in train:  []\n"
     ]
    }
   ],
   "source": [
    "# Warning: This cell takes long time\n",
    "print(\"extra in train: \",[i for i in set(Y_train.tolist()) if i not in set(medra['LLT'].tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =np.array([[encoder[i]] for i in Y_train])\n",
    "y_test = np.array([[encoder[i]] for i in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=len(max(X_ls,key=len))\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30031,), (30031, 1), (2612,), (2612, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emdlayer(window, textlst, dim):\n",
    "    '''generate embedding layer\n",
    "    random normal distribution from 0 to 0.01.\n",
    "    '''\n",
    "    v_tmp=[np.array([word_to_vec_map[i] \n",
    "                     if i in word_to_vec_map \n",
    "                     else (np.random.randn(dim,)*10000).astype('float32') for i in record[0:window]])\n",
    "           for record in textlst]\n",
    "    \n",
    "\n",
    "    vec = []\n",
    "    \n",
    "    for i in range(len(textlst)):\n",
    "        x1 = (np.random.randn(window-v_tmp[i].shape[0], dim)*10000).astype('float32')\n",
    "        x2 = v_tmp[i]\n",
    "        x = np.concatenate((x1,x2), axis=0)\n",
    "        vec.append(x)\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "#     vec = np.array([np.concatenate((v_tmp[i],\n",
    "#                                     (np.random.randn(window-v_tmp[i].shape[0], dim)*10000).astype('float32')),\n",
    "#                                    axis=0)\n",
    "#         for i in range(len(textlst))])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation of the input X_ls\n",
    "\n",
    "X_ls_perm = []\n",
    "\n",
    "for x in X_ls:\n",
    "    X_ls_perm.append(np.random.permutation(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ls_all = [*X_ls, *X_ls_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.concatenate((y_train, y_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.94 s, sys: 1.1 s, total: 5.04 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WARNING: Time consuming Cell\n",
    "# X=emdlayer(WINDOWS_Size, X_ls, 400)\n",
    "# X=emdlayer(WINDOWS_Size, X_ls_perm, 400)\n",
    "X=emdlayer(WINDOWS_Size, X_ls_all, 400)\n",
    "Xtest=emdlayer(WINDOWS_Size, X_testls, 400)\n",
    "INPUT_DIM = X.shape[2]\n",
    "SINGLE_ATTENTION_VECTOR = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60062, 6, 400), (30031, 1), (2612, 6, 400), (2612, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y_train.shape, Xtest.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.layers import concatenate, Bidirectional, Dropout, MaxPooling1D, Conv1D\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=utils.callback_()\n",
    "filename = str('./model.m0.LLT.M22.w6.6-7-2021')\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, WINDOWS_Size))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(WINDOWS_Size, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    output_attention_mul = concatenate([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(WINDOWS_Size, INPUT_DIM,))\n",
    "    ###YD 6/4/2021: Apply dropout to the input layer can dramatically control the overfit problem###\n",
    "    drop=Dropout(0.5)(inputs)\n",
    "    #########\n",
    "    lstm_units1 = 256\n",
    "    lstm_out = LSTM(lstm_units1, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01))(drop)\n",
    "#     drop2=Dropout(0.5)(lstm_out)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(CLASSES, activation='softmax')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "def model_conv_2():\n",
    "    inputs = Input(shape=(WINDOWS_Size, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    conv_0 = Conv1D(64, 7,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_0=Dropout(0.2)(conv_0)\n",
    "    conv_1 = Conv1D(64, 5,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_1=Dropout(0.2)(conv_1)\n",
    "    conv_2 = Conv1D(64, 3,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_2=Dropout(0.2)(conv_2)\n",
    "    \n",
    "    maxpool_0 = MaxPooling1D(pool_size=2)(conv_0)\n",
    "    maxpool_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "    maxpool_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "\n",
    "    conc_tensor_0 = concatenate([maxpool_0, maxpool_1], axis=1)\n",
    "    conc_tensor_1 = concatenate([maxpool_1, maxpool_2], axis=1)\n",
    "    conc_tensor_2 = concatenate([maxpool_0, maxpool_2], axis=1)\n",
    "    \n",
    "    conc_tensor_0 = Flatten()(conc_tensor_0)\n",
    "    conc_tensor_1 = Flatten()(conc_tensor_1)\n",
    "    conc_tensor_2 = Flatten()(conc_tensor_2)\n",
    "    \n",
    "    dnn_out_1=Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_tensor_0)\n",
    "    dnn_out_2=Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_tensor_1)    \n",
    "    dnn_out_3=Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_tensor_2)    \n",
    "\n",
    "    conc_dnn_out=concatenate([dnn_out_1, dnn_out_2,dnn_out_3], axis=1)\n",
    "    dnn_out_4=Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_dnn_out)  \n",
    "    dnn_out_5=Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(dnn_out_4) \n",
    "    dnn_out_6=Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(dnn_out_5) \n",
    "    output = Dense(CLASSES, activation='softmax')(dnn_out_6)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 6, 400)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 6, 400)       0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 6, 256)       672768      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 256, 6)       0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 256, 6)       0           permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256, 6)       42          reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 6, 256)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Concatenate)     (None, 6, 512)       0           lstm_2[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3072)         0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 79719)        244976487   flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 245,649,297\n",
      "Trainable params: 245,649,297\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60062 samples, validate on 2612 samples\n",
      "Epoch 1/20\n",
      "60062/60062 [==============================] - 1177s 20ms/step - loss: 8.9213 - accuracy: 0.0413 - val_loss: 8.3496 - val_accuracy: 0.1095\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.38132\n",
      "Epoch 2/20\n",
      "60062/60062 [==============================] - 1155s 19ms/step - loss: 7.2253 - accuracy: 0.1341 - val_loss: 7.4309 - val_accuracy: 0.1861\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.38132\n",
      "Epoch 3/20\n",
      "60062/60062 [==============================] - 1144s 19ms/step - loss: 5.9920 - accuracy: 0.2251 - val_loss: 6.8363 - val_accuracy: 0.2458\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.38132\n",
      "Epoch 4/20\n",
      "60062/60062 [==============================] - 1143s 19ms/step - loss: 5.0328 - accuracy: 0.3009 - val_loss: 6.5348 - val_accuracy: 0.2745\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.38132\n",
      "Epoch 5/20\n",
      "60062/60062 [==============================] - 1143s 19ms/step - loss: 4.2608 - accuracy: 0.3691 - val_loss: 6.1918 - val_accuracy: 0.3158\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.38132\n",
      "Epoch 6/20\n",
      "60062/60062 [==============================] - 1142s 19ms/step - loss: 3.6402 - accuracy: 0.4255 - val_loss: 6.0709 - val_accuracy: 0.3239\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.38132\n",
      "Epoch 7/20\n",
      "60062/60062 [==============================] - 1141s 19ms/step - loss: 3.1284 - accuracy: 0.4752 - val_loss: 5.9215 - val_accuracy: 0.3438\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.38132\n",
      "Epoch 8/20\n",
      "60062/60062 [==============================] - 1139s 19ms/step - loss: 2.7019 - accuracy: 0.5208 - val_loss: 5.8719 - val_accuracy: 0.3583\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.38132\n",
      "Epoch 9/20\n",
      "60062/60062 [==============================] - 1141s 19ms/step - loss: 2.3822 - accuracy: 0.5545 - val_loss: 5.9263 - val_accuracy: 0.3614\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.38132\n",
      "Epoch 10/20\n",
      "60062/60062 [==============================] - 1141s 19ms/step - loss: 2.1335 - accuracy: 0.5887 - val_loss: 5.9552 - val_accuracy: 0.3687\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.38132\n",
      "Epoch 11/20\n",
      "60062/60062 [==============================] - 1137s 19ms/step - loss: 1.9297 - accuracy: 0.6159 - val_loss: 6.0139 - val_accuracy: 0.3687\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.38132\n",
      "Epoch 12/20\n",
      "60062/60062 [==============================] - 1137s 19ms/step - loss: 1.7528 - accuracy: 0.6438 - val_loss: 6.0646 - val_accuracy: 0.3767\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.38132\n",
      "Epoch 13/20\n",
      "60062/60062 [==============================] - 1137s 19ms/step - loss: 1.6312 - accuracy: 0.6589 - val_loss: 6.0841 - val_accuracy: 0.3783\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.38132\n",
      "Epoch 14/20\n",
      "60062/60062 [==============================] - 1133s 19ms/step - loss: 1.5123 - accuracy: 0.6794 - val_loss: 6.2176 - val_accuracy: 0.3733\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.38132\n",
      "Epoch 15/20\n",
      "60062/60062 [==============================] - 1132s 19ms/step - loss: 1.4351 - accuracy: 0.6931 - val_loss: 6.2110 - val_accuracy: 0.3836\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.38132 to 0.38361, saving model to ./model.m0.LLT.M22.w6.6-7-2021\n",
      "Epoch 16/20\n",
      "60062/60062 [==============================] - 1140s 19ms/step - loss: 1.3566 - accuracy: 0.7059 - val_loss: 6.2902 - val_accuracy: 0.3756\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.38361\n",
      "Epoch 17/20\n",
      "60062/60062 [==============================] - 1140s 19ms/step - loss: 1.2972 - accuracy: 0.7166 - val_loss: 6.3703 - val_accuracy: 0.3897\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.38361 to 0.38974, saving model to ./model.m0.LLT.M22.w6.6-7-2021\n",
      "Epoch 18/20\n",
      "60062/60062 [==============================] - 1132s 19ms/step - loss: 1.2535 - accuracy: 0.7230 - val_loss: 6.4091 - val_accuracy: 0.3828\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.38974\n",
      "Epoch 19/20\n",
      "60062/60062 [==============================] - 1133s 19ms/step - loss: 1.2107 - accuracy: 0.7320 - val_loss: 6.4915 - val_accuracy: 0.3901\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.38974 to 0.39012, saving model to ./model.m0.LLT.M22.w6.6-7-2021\n",
      "Epoch 20/20\n",
      "60062/60062 [==============================] - 1129s 19ms/step - loss: 1.1631 - accuracy: 0.7397 - val_loss: 6.4976 - val_accuracy: 0.3890\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.39012\n",
      "CPU times: user 2d 21h 38min 4s, sys: 3h 27min 42s, total: 3d 1h 5min 46s\n",
      "Wall time: 6h 21min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras import optimizers\n",
    "m0 = model_attention_applied_after_lstm()\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "# adam = optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9)\n",
    "m0.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "m0.summary()\n",
    "\n",
    "\n",
    "h0=m0.fit([X], y_train_all, epochs=20, batch_size=128, validation_data=[[Xtest], y_test], callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = m0.predict(Xtest)\n",
    "\n",
    "####obtain LLT\n",
    "y_pred = [decoder[i] for i in y_p.argmax(axis=1)]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test.tolist(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotresult(hist, title, outputfile):\n",
    "\tacc = hist.history['accuracy']\n",
    "\tval_acc = hist.history['val_accuracy']\n",
    "\n",
    "\tepochs = len(acc)\n",
    "\tplt.plot(range(epochs), acc, marker='.', label='acc')\n",
    "\tplt.plot(range(epochs), val_acc, marker='.', label='val_acc')\n",
    "\tplt.legend(loc='best')\n",
    "\tplt.grid()\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.ylabel('acc')\n",
    "\tplt.title('Training/Validation: '+ title)\n",
    "\tplt.savefig('images/'+outputfile)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotresult(h0, '400D + LSTM + Attention', 'm0_model_attention_applied_after_lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Test AE Term \":X_test, \"Predicted AE LLT\": y_pred, \"Actual AE LLT\": Y_test}\n",
    "s = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s.loc[s[\"Predicted AE LLT\"] == s[\"Actual AE LLT\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[s[\"Predicted AE LLT\"] == s[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "s.loc[s[\"Predicted AE LLT\"] != s[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv_sm():\n",
    "    inputs = Input(shape=(WINDOWS_Size, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    conv_0 = Conv1D(64, 1,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_1 = Conv1D(64, 2,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_2 = Conv1D(64, 3,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "\n",
    "    maxpool_0 = MaxPooling1D(pool_size=3)(conv_0)\n",
    "    maxpool_1 = MaxPooling1D(pool_size=3)(conv_1)\n",
    "    maxpool_2 = MaxPooling1D(pool_size=3)(conv_2)\n",
    "\n",
    "    merged_tensor = concatenate([maxpool_0, maxpool_1,maxpool_2], axis=1)\n",
    "    lstm_out = Bidirectional(LSTM(256, activation='tanh',recurrent_regularizer=regularizers.l2(0.01),return_sequences=True))(maxpool_0)\n",
    "    merged_tensor = Flatten()(lstm_out)\n",
    "    dnn_out=Dense(64, activation=tf.nn.relu)(merged_tensor)\n",
    "    output = Dense(CLASSES, activation='softmax')(merged_tensor)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m3 = model_conv_sm()\n",
    "\n",
    "m3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "m3.summary()\n",
    "\n",
    "h1=m3.fit([X], y_train_all, epochs=35, batch_size=32, validation_data=[[Xtest], y_test], callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotresult(h1, '400D + BiLSTM + Attention', 'm1_model_attention_applied_after_bilstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h1.history['loss'])\n",
    "plt.plot(h1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./model.m0.LLT.M22.w6.6-7-2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_excel('./doc/onco-AE_coding_MedDRA v22.0_KL Review_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = test[['Verbatim Term', 'LLT Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def string_processor(x, grammer):\n",
    "        \"\"\"\n",
    "        Method to preprocess the string, includes following process:\n",
    "        1. lower case\n",
    "        2. remove punctuation\n",
    "        3. remove stop words\n",
    "        4. stem or lemmatize the word: i.e. for grammatical reasons, d documents are going to use different forms of a\n",
    "        word, such as organize, organizes, and organizing.\n",
    "        For the difference between lemmatization and stemming,\n",
    "        https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "        :param grammer: \"stem\" or \"lemma\"\n",
    "        :return: return a cleaned version of string (particularly the term in raw datasets, i.e. AETERM in AE)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            nltk.data.find('wordnet')\n",
    "        except LookupError:\n",
    "            # If it does not exist, the program downloads the stopwords.\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download(\"wordnet\", quiet=True)\n",
    "            nltk.download('stopwords', download_dir='nltk_packages', quiet=True)\n",
    "            \n",
    "        sw = stopwords.words('english')\n",
    "        # Stemming\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        # lemmatization\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        if grammer == 'stem':\n",
    "            x_cln = ' '.join([stemmer.stem(i) for i in re.sub(r'[^a-zA-Z]',' ', x).split() if i not in sw]).lower()\n",
    "        elif grammer == 'lemma':\n",
    "            x_cln = ' '.join([lemma.lemmatize(i) for i in re.sub(r'[^a-zA-Z0-9-]',' ', x).split() if i not in sw]).lower()\n",
    "        elif grammer == \"medra\":\n",
    "            x_cln = ' '.join([i.strip() for i in re.sub(r'[^a-zA-Z0-9-]',' ', x).split() if i not in sw]).lower() # keep the hyphen and numbers for the medra dictionary\n",
    "            # x_cln = ' '.join([i.strip() for i in re.sub(r'[^a-zA-Z]',' ', x).split() if i not in sw]).lower())\n",
    "        else:\n",
    "            # x_cln = ' '.join([i.strip() for i in re.sub(r'[^\\w\\s]+',' ', x).split() if i not in sw]).lower()\n",
    "            x_cln = ' '.join([i.strip() for i in re.sub(r'[^a-zA-Z0-9-]',' ', x).split() if i not in sw]).lower()\n",
    "        return x_cln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Verbatim Term'] = test['Verbatim Term'].apply(lambda x: string_processor(x, \"lemma\"))\n",
    "test['LLT Name'] = test['LLT Name'].apply(lambda x: string_processor(x, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup = test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testls_new = [w.split() for w in test['Verbatim Term']]\n",
    "len(X_testls_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt3 = 0\n",
    "WINDOWS_Size=6\n",
    "for record in X_testls_new:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            cnt3 += 1\n",
    "cnt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_new=emdlayer(WINDOWS_Size, X_testls_new, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_test = model.predict(Xtest_new)\n",
    "# y_p_test2 = test2.predict(Xtest_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = [decoder[i] for i in y_p_test.argmax(axis=1)]\n",
    "data0 = {\"Test AE Term \": test['Verbatim Term'], \"Predicted AE LLT\": y_pred_test, \"Actual AE LLT\": test['LLT Name']}\n",
    "s_0 = pd.DataFrame(data0)\n",
    "s0 = s_0.loc[s_0[\"Predicted AE LLT\"] == s_0[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test2 = [decoder[i] for i in y_p_test2.argmax(axis=1)]\n",
    "data1 = {\"Test AE Term \": new['Verbatim Term'], \"Predicted AE LLT\": y_pred_test2, \"Actual AE LLT\": new['LLT Name']}\n",
    "s_1 = pd.DataFrame(data1)\n",
    "s1 = s_1.loc[s_1[\"Predicted AE LLT\"] == s_1[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test AE Term</th>\n",
       "      <th>Predicted AE LLT</th>\n",
       "      <th>Actual AE LLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abdominal bloating</td>\n",
       "      <td>abdominal bloating</td>\n",
       "      <td>abdominal bloating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>abdominal discomfort</td>\n",
       "      <td>abdominal discomfort</td>\n",
       "      <td>abdominal discomfort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abdominal distension discomfort due to constip...</td>\n",
       "      <td>constipation</td>\n",
       "      <td>constipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>abdominal pain</td>\n",
       "      <td>abdominal pain</td>\n",
       "      <td>abdominal pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>acne</td>\n",
       "      <td>acne</td>\n",
       "      <td>acne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>pneumonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>slurring of speech</td>\n",
       "      <td>slurred speech</td>\n",
       "      <td>slurred speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>thromboembolic event pulmonary embolism</td>\n",
       "      <td>pulmonary embolism</td>\n",
       "      <td>pulmonary embolism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>vomiting intermittent</td>\n",
       "      <td>vomiting</td>\n",
       "      <td>vomiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>wbc decreased</td>\n",
       "      <td>wbc decreased</td>\n",
       "      <td>wbc decreased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Test AE Term       Predicted AE LLT  \\\n",
       "11                                   abdominal bloating    abdominal bloating   \n",
       "12                                 abdominal discomfort  abdominal discomfort   \n",
       "15    abdominal distension discomfort due to constip...          constipation   \n",
       "17                                       abdominal pain        abdominal pain   \n",
       "38                                                 acne                  acne   \n",
       "...                                                 ...                   ...   \n",
       "1296                                          pneumonia             pneumonia   \n",
       "1301                                 slurring of speech        slurred speech   \n",
       "1307            thromboembolic event pulmonary embolism    pulmonary embolism   \n",
       "1311                              vomiting intermittent              vomiting   \n",
       "1312                                      wbc decreased         wbc decreased   \n",
       "\n",
       "             Actual AE LLT  \n",
       "11      abdominal bloating  \n",
       "12    abdominal discomfort  \n",
       "15            constipation  \n",
       "17          abdominal pain  \n",
       "38                    acne  \n",
       "...                    ...  \n",
       "1296             pneumonia  \n",
       "1301        slurred speech  \n",
       "1307    pulmonary embolism  \n",
       "1311              vomiting  \n",
       "1312         wbc decreased  \n",
       "\n",
       "[534 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4067022086824067"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc0 = len(s0)/len(test)\n",
    "acc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc1 = len(s1)/len(new)\n",
    "# acc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for new test data is with model 1 is 40.67022086824067%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy for new test data is with model 1 is \" + str(acc0 * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Test accuracy for new test data is with model 2 is \" + str(acc1 * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_0.to_excel(\"output.xlsx\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
