{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import importlib\n",
    "warnings.filterwarnings('ignore')\n",
    "from loader import Loader\n",
    "from preprocessor import Preprocessor\n",
    "from spliter import Spliter \n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "# file_name = [\"data/ae.sas7bdat\"]\n",
    "# dict_name = [\"data/meddra_dict_v21\",\"data/meddra_dict_v22\", \"data/meddra_dict_v23\"]\n",
    "dict_name = [\"data/meddra_dict_v22\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verbatim Term</th>\n",
       "      <th>LLT Name</th>\n",
       "      <th>Version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contact dermatitis</td>\n",
       "      <td>contact dermatitis</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cataracts</td>\n",
       "      <td>cataracts</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occasional lightheadedness</td>\n",
       "      <td>lightheadedness</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>occassional neurologic dizziness</td>\n",
       "      <td>dizziness</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swollen lymph nodes bilateral neck</td>\n",
       "      <td>swollen lymph nodes</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32768</th>\n",
       "      <td>right hemicolectomy</td>\n",
       "      <td>right hemicolectomy</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32769</th>\n",
       "      <td>hepatic lobectomy</td>\n",
       "      <td>liver lobectomy</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32770</th>\n",
       "      <td>colon biopsy</td>\n",
       "      <td>colon biopsy</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32771</th>\n",
       "      <td>laparoscopic low anterior resection</td>\n",
       "      <td>lower anterior resection</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32772</th>\n",
       "      <td>cat scan guided biopsy</td>\n",
       "      <td>biopsy lung</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32773 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Verbatim Term                  LLT Name  Version\n",
       "0                       contact dermatitis        contact dermatitis     17.0\n",
       "1                                cataracts                 cataracts     17.0\n",
       "2               occasional lightheadedness           lightheadedness     17.0\n",
       "3         occassional neurologic dizziness                 dizziness     17.0\n",
       "4       swollen lymph nodes bilateral neck       swollen lymph nodes     17.0\n",
       "...                                    ...                       ...      ...\n",
       "32768                  right hemicolectomy       right hemicolectomy     23.0\n",
       "32769                    hepatic lobectomy           liver lobectomy     23.0\n",
       "32770                         colon biopsy              colon biopsy     23.0\n",
       "32771  laparoscopic low anterior resection  lower anterior resection     23.0\n",
       "32772               cat scan guided biopsy               biopsy lung     23.0\n",
       "\n",
       "[32773 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.rawdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Preprocessor(loader.rawdf, loader.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "medra,raw = processor.pipe_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TERM</th>\n",
       "      <th>LLT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contact dermatitis</td>\n",
       "      <td>contact dermatitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cataract</td>\n",
       "      <td>cataracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occasional lightheadedness</td>\n",
       "      <td>lightheadedness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>occassional neurologic dizziness</td>\n",
       "      <td>dizziness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swollen lymph node bilateral neck</td>\n",
       "      <td>swollen lymph nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32638</th>\n",
       "      <td>abdominal hysterectomy</td>\n",
       "      <td>abdominal hysterectomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32639</th>\n",
       "      <td>right hemicolectomy</td>\n",
       "      <td>right hemicolectomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32640</th>\n",
       "      <td>hepatic lobectomy</td>\n",
       "      <td>liver lobectomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32641</th>\n",
       "      <td>colon biopsy</td>\n",
       "      <td>colon biopsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32642</th>\n",
       "      <td>cat scan guided biopsy</td>\n",
       "      <td>biopsy lung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32643 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TERM                     LLT\n",
       "0                     contact dermatitis      contact dermatitis\n",
       "1                               cataract               cataracts\n",
       "2             occasional lightheadedness         lightheadedness\n",
       "3       occassional neurologic dizziness               dizziness\n",
       "4      swollen lymph node bilateral neck     swollen lymph nodes\n",
       "...                                  ...                     ...\n",
       "32638             abdominal hysterectomy  abdominal hysterectomy\n",
       "32639                right hemicolectomy     right hemicolectomy\n",
       "32640                  hepatic lobectomy         liver lobectomy\n",
       "32641                       colon biopsy            colon biopsy\n",
       "32642             cat scan guided biopsy             biopsy lung\n",
       "\n",
       "[32643 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, X_ls, X_testls = Spliter(raw, medra).get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              exacerbation herpes simplex\n",
       "1           painful - l foot joint big toe\n",
       "2         high creatinine level 106 umol l\n",
       "3            absent reflex lower extremity\n",
       "4        interstitial nodular opacity lung\n",
       "                       ...                \n",
       "30026                        uti infection\n",
       "30027                     burnt right hand\n",
       "30028                       hypomagnasemia\n",
       "30029                            sore limb\n",
       "30030                   allergy penicillin\n",
       "Name: TERM, Length: 30031, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30031,), (2612,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 6s, sys: 18.1 s, total: 7min 24s\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WARNING: Time consuming\n",
    "word_to_index, index_to_word, word_to_vec_map = utils.read_emb_vecs('./ri-3gram-400-tsv/vocab.tsv', './ri-3gram-400-tsv/vectors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CLASSES=len(set(medra['LLT']))\n",
    "WINDOWS_Size=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count potential spelling errors or words cannot be found in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for record in X_ls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7752 number of individual training words NOT found in the word embedding vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(cnt) + \" number of individual training words NOT found in the word embedding vectors\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 51s, sys: 1.07 s, total: 16min 52s\n",
      "Wall time: 16min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WARNING: Time consuming cell\n",
    "from spellchecker import SpellChecker \n",
    "spell = SpellChecker()\n",
    "for record in X_ls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            idx = record.index(i)\n",
    "            record[idx] = spell.correction(i)\n",
    "            \n",
    "for record in X_testls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            idx = record.index(i)\n",
    "            record[idx] = spell.correction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt2 = 0\n",
    "for record in X_ls:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "#             print(i)\n",
    "            cnt2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5501 number of individual training words NOT found after spell correction and other corrections\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(cnt2) + \" number of individual training words NOT found after spell correction and other corrections\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {}\n",
    "for i, pt in enumerate(set(medra['LLT'])):\n",
    "    encoder.update({pt: i})\n",
    "    i = i + 1\n",
    "decoder = dict([(pt, i) for i, pt in encoder.items()])\n",
    "\n",
    "outfile = open(os.path.join('data', 'lltcoder.pkl'),'wb')\n",
    "pickle.dump(encoder,outfile)\n",
    "outfile.close()\n",
    "outfile = open(os.path.join('data', 'lltdecoder.pkl'),'wb')\n",
    "pickle.dump(decoder,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code are designed to test if there are any train/test not in the target meddra version. Even though, using different version to train the model, it not makes sense to includes every version in the output since the output is version specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra in test:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"extra in test: \",[i for i in set(Y_test.tolist()) if i not in set(medra['LLT'].tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra in train:  []\n"
     ]
    }
   ],
   "source": [
    "# Warning: This cell takes long time\n",
    "print(\"extra in train: \",[i for i in set(Y_train.tolist()) if i not in set(medra['LLT'].tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =np.array([[encoder[i]] for i in Y_train])\n",
    "y_test = np.array([[encoder[i]] for i in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=len(max(X_ls,key=len))\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30031,), (30031, 1), (2612,), (2612, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emdlayer(window, textlst, dim):\n",
    "    '''generate embedding layer\n",
    "    random normal distribution from 0 to 0.01.\n",
    "    '''\n",
    "    v_tmp=[np.array([word_to_vec_map[i] \n",
    "                     if i in word_to_vec_map \n",
    "                     else (np.random.randn(dim,)*10000).astype('float32') for i in record[0:window]])\n",
    "           for record in textlst]\n",
    "    \n",
    "\n",
    "    vec = []\n",
    "    \n",
    "    for i in range(len(textlst)):\n",
    "        x1 = (np.random.randn(window-v_tmp[i].shape[0], dim)*10000).astype('float32')\n",
    "        x2 = v_tmp[i]\n",
    "        x = np.concatenate((x1,x2), axis=0)\n",
    "        vec.append(x)\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "#     vec = np.array([np.concatenate((v_tmp[i],\n",
    "#                                     (np.random.randn(window-v_tmp[i].shape[0], dim)*10000).astype('float32')),\n",
    "#                                    axis=0)\n",
    "#         for i in range(len(textlst))])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation of the input X_ls\n",
    "\n",
    "X_ls_perm = []\n",
    "\n",
    "for x in X_ls:\n",
    "    X_ls_perm.append(np.random.permutation(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ls_all = [*X_ls, *X_ls_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.concatenate((y_train, y_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 s, sys: 1.05 s, total: 5.05 s\n",
      "Wall time: 5.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WARNING: Time consuming Cell\n",
    "# X=emdlayer(WINDOWS_Size, X_ls, 400)\n",
    "# X=emdlayer(WINDOWS_Size, X_ls_perm, 400)\n",
    "X=emdlayer(WINDOWS_Size, X_ls_all, 400)\n",
    "Xtest=emdlayer(WINDOWS_Size, X_testls, 400)\n",
    "INPUT_DIM = X.shape[2]\n",
    "SINGLE_ATTENTION_VECTOR = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60062, 6, 400), (30031, 1), (2612, 6, 400), (2612, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y_train.shape, Xtest.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.layers import concatenate, Bidirectional, Dropout, MaxPooling1D, Conv1D\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=utils.callback_()\n",
    "filename = str('./model.m0.LLT.M22.w6.6-5-2021(test)')\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, WINDOWS_Size))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(WINDOWS_Size, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    output_attention_mul = concatenate([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(WINDOWS_Size, INPUT_DIM,))\n",
    "    ###YD 6/4/2021: Apply dropout to the input layer can dramatically control the overfit problem###\n",
    "    ### but an important question here is the dropout rate. it shows 0.3 > 0.2####\n",
    "    drop=Dropout(0.4)(inputs)\n",
    "    #########\n",
    "    lstm_units1 = 256\n",
    "    lstm_out = LSTM(lstm_units1, return_sequences=True, recurrent_regularizer=regularizers.l2(0.01))(drop)\n",
    "#     drop2=Dropout(0.5)(lstm_out)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(CLASSES, activation='softmax')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "\n",
    "def model_conv_2():\n",
    "    inputs = Input(shape=(WINDOWS_Size, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    conv_0 = Conv1D(64, 7,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_0=Dropout(0.2)(conv_0)\n",
    "    conv_1 = Conv1D(64, 5,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_1=Dropout(0.2)(conv_1)\n",
    "    conv_2 = Conv1D(64, 3,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_2=Dropout(0.2)(conv_2)\n",
    "    \n",
    "    maxpool_0 = MaxPooling1D(pool_size=2)(conv_0)\n",
    "    maxpool_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "    maxpool_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "\n",
    "    conc_tensor_0 = concatenate([maxpool_0, maxpool_1], axis=1)\n",
    "    conc_tensor_1 = concatenate([maxpool_1, maxpool_2], axis=1)\n",
    "    conc_tensor_2 = concatenate([maxpool_0, maxpool_2], axis=1)\n",
    "    \n",
    "    conc_tensor_0 = Flatten()(conc_tensor_0)\n",
    "    conc_tensor_1 = Flatten()(conc_tensor_1)\n",
    "    conc_tensor_2 = Flatten()(conc_tensor_2)\n",
    "    \n",
    "    dnn_out_1=Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_tensor_0)\n",
    "    dnn_out_2=Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_tensor_1)    \n",
    "    dnn_out_3=Dense(256, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_tensor_2)    \n",
    "\n",
    "    conc_dnn_out=concatenate([dnn_out_1, dnn_out_2,dnn_out_3], axis=1)\n",
    "    dnn_out_4=Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(conc_dnn_out)  \n",
    "    dnn_out_5=Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(dnn_out_4) \n",
    "    dnn_out_6=Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.01))(dnn_out_5) \n",
    "    output = Dense(CLASSES, activation='softmax')(dnn_out_6)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras import optimizers\n",
    "m0 = model_attention_applied_after_lstm()\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "# adam = optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9)\n",
    "m0.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "m0.summary()\n",
    "\n",
    "\n",
    "h0=m0.fit([X], y_train_all, epochs=20, batch_size=128, validation_data=[[Xtest], y_test], callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = m0.predict(Xtest)\n",
    "\n",
    "####obtain LLT\n",
    "y_pred = [decoder[i] for i in y_p.argmax(axis=1)]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test.tolist(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotresult(hist, title, outputfile):\n",
    "\tacc = hist.history['accuracy']\n",
    "\tval_acc = hist.history['val_accuracy']\n",
    "\n",
    "\tepochs = len(acc)\n",
    "\tplt.plot(range(epochs), acc, marker='.', label='acc')\n",
    "\tplt.plot(range(epochs), val_acc, marker='.', label='val_acc')\n",
    "\tplt.legend(loc='best')\n",
    "\tplt.grid()\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.ylabel('acc')\n",
    "\tplt.title('Training/Validation: '+ title)\n",
    "\tplt.savefig('images/'+outputfile)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotresult(h0, '400D + LSTM + Attention', 'm0_model_attention_applied_after_lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Test AE Term \":X_test, \"Predicted AE LLT\": y_pred, \"Actual AE LLT\": Y_test}\n",
    "s = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s.loc[s[\"Predicted AE LLT\"] == s[\"Actual AE LLT\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loc[s[\"Predicted AE LLT\"] == s[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "s.loc[s[\"Predicted AE LLT\"] != s[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv_sm():\n",
    "    inputs = Input(shape=(WINDOWS_Size, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    conv_0 = Conv1D(64, 1,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_1 = Conv1D(64, 2,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "    conv_2 = Conv1D(64, 3,activation='relu',kernel_regularizer=regularizers.l2(0.01))(attention_mul)\n",
    "\n",
    "    maxpool_0 = MaxPooling1D(pool_size=3)(conv_0)\n",
    "    maxpool_1 = MaxPooling1D(pool_size=3)(conv_1)\n",
    "    maxpool_2 = MaxPooling1D(pool_size=3)(conv_2)\n",
    "\n",
    "    merged_tensor = concatenate([maxpool_0, maxpool_1,maxpool_2], axis=1)\n",
    "    lstm_out = Bidirectional(LSTM(256, activation='tanh',recurrent_regularizer=regularizers.l2(0.01),return_sequences=True))(maxpool_0)\n",
    "    merged_tensor = Flatten()(lstm_out)\n",
    "    dnn_out=Dense(64, activation=tf.nn.relu)(merged_tensor)\n",
    "    output = Dense(CLASSES, activation='softmax')(merged_tensor)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m3 = model_conv_sm()\n",
    "\n",
    "m3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "m3.summary()\n",
    "\n",
    "h1=m3.fit([X], y_train_all, epochs=35, batch_size=32, validation_data=[[Xtest], y_test], callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotresult(h1, '400D + BiLSTM + Attention', 'm1_model_attention_applied_after_bilstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h1.history['loss'])\n",
    "plt.plot(h1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "test = keras.models.load_model('./model.m0.LLT.M22.w6.6-4-2021-dropout-0.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_excel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = test[['Verbatim Term', 'LLT Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def string_processor(x, grammer):\n",
    "        \"\"\"\n",
    "        Method to preprocess the string, includes following process:\n",
    "        1. lower case\n",
    "        2. remove punctuation\n",
    "        3. remove stop words\n",
    "        4. stem or lemmatize the word: i.e. for grammatical reasons, d documents are going to use different forms of a\n",
    "        word, such as organize, organizes, and organizing.\n",
    "        For the difference between lemmatization and stemming,\n",
    "        https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "        :param grammer: \"stem\" or \"lemma\"\n",
    "        :return: return a cleaned version of string (particularly the term in raw datasets, i.e. AETERM in AE)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            nltk.data.find('wordnet')\n",
    "        except LookupError:\n",
    "            # If it does not exist, the program downloads the stopwords.\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download(\"wordnet\", quiet=True)\n",
    "            nltk.download('stopwords', download_dir='nltk_packages', quiet=True)\n",
    "            \n",
    "        sw = stopwords.words('english')\n",
    "        # Stemming\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        # lemmatization\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        if grammer == 'stem':\n",
    "            x_cln = ' '.join([stemmer.stem(i) for i in re.sub(r'[^a-zA-Z]',' ', x).split() if i not in sw]).lower()\n",
    "        elif grammer == 'lemma':\n",
    "            x_cln = ' '.join([lemma.lemmatize(i) for i in re.sub(r'[^a-zA-Z0-9-]',' ', x).split() if i not in sw]).lower()\n",
    "        elif grammer == \"medra\":\n",
    "            x_cln = ' '.join([i.strip() for i in re.sub(r'[^a-zA-Z0-9-]',' ', x).split() if i not in sw]).lower() # keep the hyphen and numbers for the medra dictionary\n",
    "            # x_cln = ' '.join([i.strip() for i in re.sub(r'[^a-zA-Z]',' ', x).split() if i not in sw]).lower())\n",
    "        else:\n",
    "            # x_cln = ' '.join([i.strip() for i in re.sub(r'[^\\w\\s]+',' ', x).split() if i not in sw]).lower()\n",
    "            x_cln = ' '.join([i.strip() for i in re.sub(r'[^a-zA-Z0-9-]',' ', x).split() if i not in sw]).lower()\n",
    "        return x_cln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Verbatim Term'] = test['Verbatim Term'].apply(lambda x: string_processor(x, \"lemma\"))\n",
    "test['LLT Name'] = test['LLT Name'].apply(lambda x: string_processor(x, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup = new.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testls_new = [w.split() for w in new['Verbatim Term']]\n",
    "len(X_testls_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt3 = 0\n",
    "WINDOWS_Size=6\n",
    "for record in X_testls_new:\n",
    "    for i in record[0:WINDOWS_Size]:\n",
    "        if i not in word_to_vec_map:\n",
    "            cnt3 += 1\n",
    "cnt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_new=emdlayer(WINDOWS_Size, X_testls_new, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_test = test.predict(Xtest_new)\n",
    "# y_p_test2 = test2.predict(Xtest_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = [decoder[i] for i in y_p_test.argmax(axis=1)]\n",
    "data0 = {\"Test AE Term \": new['Verbatim Term'], \"Predicted AE LLT\": y_pred_test, \"Actual AE LLT\": new['LLT Name']}\n",
    "s_0 = pd.DataFrame(data0)\n",
    "s0 = s_0.loc[s_0[\"Predicted AE LLT\"] == s_0[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test2 = [decoder[i] for i in y_p_test2.argmax(axis=1)]\n",
    "data1 = {\"Test AE Term \": new['Verbatim Term'], \"Predicted AE LLT\": y_pred_test2, \"Actual AE LLT\": new['LLT Name']}\n",
    "s_1 = pd.DataFrame(data1)\n",
    "s1 = s_1.loc[s_1[\"Predicted AE LLT\"] == s_1[\"Actual AE LLT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc0 = len(s0)/len(new)\n",
    "acc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = len(s1)/len(new)\n",
    "acc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy for new test data is with model 1 is \" + str(acc0 * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy for new test data is with model 2 is \" + str(acc1 * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_0.to_excel(\"output.xlsx\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
